---
title: "Statistical analyses"
author: "Author 1"
date: "2023-10-24"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
Sys.setenv(
  CC    = "clang",
  CXX   = "clang++",
  CXX11 = "clang++",
  CXX14 = "clang++",
  CXX17 = "clang++"
)
Sys.setenv(SDKROOT = system("/usr/bin/xcrun --show-sdk-path", intern = TRUE))

# cmdstanr/rstan/brms preferences
options(brms.backend = "cmdstanr")

options(mc.cores = parallel::detectCores())
rstan::rstan_options(auto_write = TRUE)

options(max.print=1e6)                  # Print up to a million entries!
options(knitr.kable.NA = '')            # For better table handling (optional)
knitr::opts_chunk$set(
  echo = TRUE,
  warning = TRUE,
  message = TRUE,
  max.lines = 1e6                       # Ensures chunk output isn't truncated
)
```

<br>
Hello, reader! Thank you for your interest in this work.

Let me guide you through the steps taken to conduct the statistical analyses reported on our paper.

## Preparation

**Loading libraries**

Here are the libraries that we are going to employ. If you have not already, remember to install them via `install.packages()` first:

```{r load libraries, message=FALSE}
library(tidyverse)
library(corrplot)
library(ggstatsplot)
library(lme4)
library(caret)
library(car)
library(broom)
library(glmmTMB)
library(DHARMa)
library(mosaic)
library(MASS)
library(nlme)
library(nnet)
library(lmtest)
library(glmnet)
library(visreg)
library(mgcv)
library(ggeffects)
library(ROCR)
library(broom.mixed)
library(boot)
library(parameters)
library(sandwich)
library(clubSandwich)
library(multiwayvcov)
library(pscl)
library(ResourceSelection)
library(pROC)
library(cmdstanr)
library(brms)
library(patchwork)
library(bayesplot)
library(future)
```
<br>
<br>
**Loading data sets**

Next, we are going to load the data sets obtained after computing the measures needed for our model with Python.

```{r load files, message=FALSE}
# ==== 1. STUDENT PREFERENCES ====
student_preferences <- read_csv('Data/participant_responses.csv')

# ==== 2. TEXT AND FULL ERROR ANALYSES ====
full_text_analysis <- read_csv('Data/full_measures.csv')
full_error_measures <- read_csv('Data/full_error_measures.csv')

# ==== 3. TEXT AND TRACKED ERROR ANALYSIS ====
tracked_text_analysis <- read_csv('Data/tracked_error_measures.csv')
```

<br>
<br>
**RQ1: IN WHAT WAYS DO DIFFERENT TYPES OF WCF INFLUENCE THE LEXICAL ACCURACY OF ADVANCED L2 SPANISH LEARNERS? HOW, IF AT ALL, DOES THIS AFFECT LEXICAL COMPLEXITY?**
<br>

```{r rq1 analyses, message=FALSE, fig.width=12, fig.height=6}

# 1.1. === DATA PREPARATION ===

full_text_analysis <- full_join(full_text_analysis, full_error_measures, by = "error", relationship = "many-to-many")
rows_to_remove <- c(65, 67, 187, 195)
full_text_analysis <- full_text_analysis[-rows_to_remove,]

lmm.df <- full_text_analysis %>% dplyr::select(participant_ID = participant_ID.x,
                                               text_ID = text_ID.x,
                                               TextNum = TextNum.x,
                                               fb_type = fb_type.x,
                                               word_count = Total,
                                               lexical_density = `Lexical density`,
                                               FreqC1, FreqC2, rttr, mtld, error_rate, error_count)

lmm.df.wide <- lmm.df %>%
  group_by(text_ID, fb_type) %>%
  summarise(count = n()) %>%
  pivot_wider(names_from = fb_type, values_from = count,
              names_prefix = "fb_count_")

lmm.df.wide[is.na(lmm.df.wide)] <- 0

lmm.df <- full_join(lmm.df, lmm.df.wide, by = "text_ID")

lmm.df <- lmm.df %>%
  mutate(FreqC = FreqC1 + FreqC2) %>%
  dplyr::select(-text_ID, -fb_type, -FreqC1, -FreqC2)

lmm.df <- distinct(lmm.df)

lmm.df <- lmm.df %>%
  arrange(participant_ID, TextNum) %>%
  group_by(participant_ID) %>%
  mutate(across(starts_with("fb_count_"), \(x) lag(x, default = 0))) %>%
  ungroup() %>%
  arrange(participant_ID, TextNum)

lmm.df <- lmm.df %>%
  mutate(across(starts_with("fb_count_"), ~ . / word_count, .names = "rate_{.col}"))

lmm.df <- lmm.df %>%
  dplyr::select(-c("fb_count_D.M.":"fb_count_R"))

lmm.df$participant_ID <- as.factor(lmm.df$participant_ID)
lmm.df$TextNum <- as.factor(lmm.df$TextNum)

lmm.df$rate_fb_count_D.M. <- scale(lmm.df$rate_fb_count_D.M.)
lmm.df$rate_fb_count_D.N.M <- scale(lmm.df$rate_fb_count_D.N.M)
lmm.df$rate_fb_count_I.L. <- scale(lmm.df$rate_fb_count_I.L.)
lmm.df$rate_fb_count_I.M. <- scale(lmm.df$rate_fb_count_I.M.)
lmm.df$rate_fb_count_None <- scale(lmm.df$rate_fb_count_None)
lmm.df$rate_fb_count_R <- scale(lmm.df$rate_fb_count_R)


# 1.2. === GENERALIZED LINEAR MIXED EFFECTS MODEL WITH BETA DISTRIBUTION ===

# 1.2.1. Fitting and interpreting the model

model.RQ1.errorRate <- glmmTMB(
  error_rate ~ rate_fb_count_D.M. + rate_fb_count_D.N.M + rate_fb_count_I.L. + rate_fb_count_I.M. + rate_fb_count_R + rate_fb_count_None + TextNum + (1|participant_ID),
  data = lmm.df, family = beta_family())

summary(model.RQ1.errorRate)

model_parameters(model.RQ1.errorRate)
model_parameters(model.RQ1.errorRate, exponentiate = TRUE)


# 1.2.2 Checking for overdispersion
overdisp <- residuals(model.RQ1.errorRate, type = "pearson")^2
overdisp_ratio <- sum(overdisp) / df.residual(model.RQ1.errorRate)
overdisp_ratio

car::vif(lm(error_rate ~ rate_fb_count_D.M. + rate_fb_count_D.N.M + rate_fb_count_I.L. + rate_fb_count_I.M. + rate_fb_count_None + rate_fb_count_R + TextNum, data = lmm.df))


# 1.2.3 Visualizing fitted values vs. observed values
#plot(fitted(model.RQ1.errorRate), lmm.df$error_rate, xlab = "Fitted Values", ylab = "Observed Values")

# 1.2.4. Checking residuals vs. fitted values
#plot(fitted(model.RQ1.errorRate), resid(model.RQ1.errorRate), xlab = "Fitted Values", ylab = "Residuals")

# 1.2.5. Checking the Q-Q plot of the residuals
#qqnorm(resid(model.RQ1.errorRate))
#qqline(resid(model.RQ1.errorRate))

# 1.2.6. Interpreting residuals

##   Florian Hartig (2022). DHARMa: Residual Diagnostics for Hierarchical
##   (Multi-Level / Mixed) Regression Models. R package version 0.4.6.
##   http://florianhartig.github.io/DHARMa/

par(mfrow = c(1, 2))
a <- plotQQunif(model.RQ1.errorRate)
b <- plotResiduals(model.RQ1.errorRate)
par(mfrow = c(1, 1))

# 1.3. === PERMUTATION TESTS ON THE EQUALITY OF MEANS OF LEXICAL COMPLEXITY MEASURES ===

# 1.3.1. Calculate the observed difference in means

avg.FreqC <- mean(FreqC ~ TextNum, data = lmm.df) %>%
  diff()
avg.mtld <- mean(mtld ~ TextNum, data = lmm.df) %>%
  diff()
avg.density <- mean(lexical_density ~ TextNum, data = lmm.df) %>%
  diff()

# 1.3.2. Create a randomization distribution of all the differences in means under the null hypothesis
set.seed(17)
FreqC_null <- do(10000) * mean(FreqC ~ shuffle(TextNum), data = lmm.df) %>%
  diff()
set.seed(17)
mtld_null <- do(10000) * mean(mtld ~ shuffle(TextNum), data = lmm.df) %>%
  diff()
set.seed(17)
density_null <- do(10000) * mean(lexical_density ~ shuffle(TextNum), data = lmm.df) %>%
  diff()

# 1.3.3. Plot the randomization distribution
p <- ggplot(data = FreqC_null) +
  geom_histogram(mapping = aes(x = X2), alpha = 0.5, colour = "white") +
  geom_histogram(mapping = aes(x = X3), alpha = 0.5, colour = "green") +
  geom_histogram(mapping = aes(x = X4), alpha = 0.5, colour = "purple") +
  xlab("mean difference")

q <- ggplot(data = mtld_null) +
  geom_histogram(mapping = aes(x = X2), alpha = 0.5, colour = "white") +
  geom_histogram(mapping = aes(x = X3), alpha = 0.5, colour = "green") +
  geom_histogram(mapping = aes(x = X4), alpha = 0.5, colour = "purple") +
  xlab("mean difference")

r <- ggplot(data = density_null) +
  geom_histogram(mapping = aes(x = X2), alpha = 0.5, colour = "white") +
  geom_histogram(mapping = aes(x = X3), alpha = 0.5, colour = "green") +
  geom_histogram(mapping = aes(x = X4), alpha = 0.5, colour = "purple") +
  xlab("mean difference")

# 1.3.4. Superimpose a line indicating the observation
p + geom_vline(xintercept = avg.FreqC, linetype = 2, colour = "blue")
q + geom_vline(xintercept = avg.mtld, linetype = 2, colour = "blue")
r + geom_vline(xintercept = avg.density, linetype = 2, colour = "blue")

# 1.3.6. Calculate the proportion of simulated differences in means as or more extreme than the observed

prop_FreqC_X2 <- mean(FreqC_null$X2 >= avg.FreqC["2"])
prop_FreqC_X3 <- mean(FreqC_null$X3 >= avg.FreqC["3"])
prop_FreqC_X4 <- mean(FreqC_null$X4 >= avg.FreqC["4"])

prop_mtld_X2 <- mean(mtld_null$X2 >= avg.mtld["2"])
prop_mtld_X3 <- mean(mtld_null$X3 >= avg.mtld["3"])
prop_mtld_X4 <- mean(mtld_null$X4 >= avg.mtld["4"])

prop_density_X2 <- mean(density_null$X2 >= avg.density["2"])
prop_density_X3 <- mean(density_null$X3 >= avg.density["3"])
prop_density_X4 <- mean(density_null$X4 >= avg.density["4"])

# 1.3.7. Apply BH correction

p_values_FreqC <- c(prop_FreqC_X2, prop_FreqC_X3, prop_FreqC_X4)
p_values_mtld <- c(prop_mtld_X2, prop_mtld_X3, prop_mtld_X4)
p_values_density <- c(prop_density_X2, prop_density_X3, prop_density_X4)

p_values_FreqC_corrected <- p.adjust(p_values_FreqC, method = "BH", n = length(p_values_FreqC))
p_values_mtld_corrected <- p.adjust(p_values_mtld, method = "BH", n = length(p_values_mtld))
p_values_density_corrected <- p.adjust(p_values_density, method = "BH", n = length(p_values_density))

p_values_FreqC_corrected
p_values_mtld_corrected
p_values_density_corrected
```

<br>
<br>
<br>
**RQ2: HOW DO DIFFERENT WCF FORMATS INTERACT WITH VARIOUS TYPES OF LEXICAL ERRORS, AND WHAT PATTERNS EMERGE IN LEARNERS' RESPONSES, INCLUDING ERROR CORRECTION OR REPETITION?**

```{r rq2, message=FALSE, fig.width=12, fig.height=6}

# === 2.1. DATA PREPARATION ===

# 2.1.1. Join the error and text analyses with the questionnaire responses
preferences_and_performance <- full_join(student_preferences, tracked_text_analysis, by = "participant_ID")


# 2.1.2. Keep only relevant columns
preferences_and_performance <- preferences_and_performance[, c("participant_ID",
                                                               "age",
                                                               "sex",
                                                               "country",
                                                               "background",
                                                               "years_speak_spa",
                                                               "fb_importance",
                                                               "fb_satisfaction",
                                                               "freq_seek_additional_info",
                                                               "how_much_fb_helps_learn",
                                                               "fb_1_clarity", "fb_1_usefulness", "likes_fb_1",
                                                               "fb_2_clarity", "fb_2_usefulness", "likes_fb_2",
                                                               "fb_3_clarity", "fb_3_usefulness", "likes_fb_3",
                                                               "fb_4_clarity", "fb_4_usefulness", "likes_fb_4",
                                                               "fb_5_clarity", "fb_5_usefulness", "likes_fb_5",
                                                               "fb_6_clarity", "fb_6_usefulness", "likes_fb_6",
                                                               "fb_7_clarity", "fb_7_usefulness", "likes_fb_7",
                                                               "text_ID",
                                                               "error",
                                                               "error_type",
                                                               "fb_type",
                                                               "pass_solution_reps", "fail_solution_reps", "error_reps",
                                                               "TextNum",
                                                               "error_rate", "error_rate_per_type", "fb_rate_per_type",
                                                               "FreqA1", "FreqA2", "FreqB1", "FreqB2", "FreqC1", "FreqC2",
                                                               "msttr", "mattr", "ttr", "rttr", "cttr",
                                                               "mtld", "hdd", "Herdan", "Summer", "Dugast", "Maas",
                                                               "outcome")]


# === 2.2. LASSO REGRESSION ===

# 2.2.1. Collapse the "Both fixed and repeated" and "Fixed inadequately" levels to simplify the model and upsample the data to address class imbalance

preferences_and_performance$outcome[preferences_and_performance$outcome == "Both fixed and repeated"] <- "Fixed"
preferences_and_performance$outcome[preferences_and_performance$outcome == "Both fixed and repeated"] <- "Repeated"
preferences_and_performance$outcome[preferences_and_performance$outcome == "Fixed inadequately"] <- "Repeated"

preferences_and_performance$fb_type <- as.factor(preferences_and_performance$fb_type)
preferences_and_performance$fb_type <- relevel(preferences_and_performance$fb_type, ref = "No WCF")
preferences_and_performance$error_type <- as.factor(preferences_and_performance$error_type)
preferences_and_performance$outcome <- as.factor(preferences_and_performance$outcome)
preferences_and_performance$participant_ID <- as.factor(preferences_and_performance$participant_ID)

set.seed(17)
up.preferences_and_performance <- upSample(preferences_and_performance[,-60],
                                           preferences_and_performance$outcome,
                                           yname="outcome")

up.preferences_and_performance <- up.preferences_and_performance %>%
  dplyr::select(outcome, fb_type, error_type, participant_ID)

# 2.2.2. Fit the model with the appropriate lambda using cross-validation

lasso.data <- up.preferences_and_performance
lasso.data$fb_type <- relevel(lasso.data$fb_type, ref = "No WCF")

lasso.data$outcome_binary <- ifelse(lasso.data$outcome == "Fixed", 1, 0)

X <- model.matrix(~ error_type * fb_type, data = lasso.data)

CV.model.RQ2.outcome <- cv.glmnet(X,
                                  lasso.data$outcome_binary,
                                  family = "binomial",
                                  alpha = 1)
par(mfrow = c(1, 2))
plot(CV.model.RQ2.outcome)
plot(CV.model.RQ2.outcome$glmnet.fit)
par(mfrow = c(1, 1))
lambda.1se <- CV.model.RQ2.outcome$lambda.1se

model.RQ2.outcome <- glmnet(X,
                            lasso.data$outcome_binary,
                            alpha = 1,
                            lambda = lambda.1se)

lasso_coeffs <- coef(model.RQ2.outcome, s = "lambda.1se")
plot(lasso_coeffs)

model.RQ2.outcome
lasso_coeffs

new_X <- expand.grid(
  error_type = levels(lasso.data$error_type),
  fb_type = levels(lasso.data$fb_type)
)

# Number of parameters (non-zero coefficients in the model)
num_parameters <- sum(coef(model.RQ2.outcome) != 0)

# Deviance of the model
deviance <- deviance(model.RQ2.outcome)

# Number of observations
n_obs <- nobs(model.RQ2.outcome)

# Calculate AIC and BIC
AIC_value <- deviance + 2 * num_parameters
BIC_value <- deviance + log(n_obs) * num_parameters

# Likelihood ratio test for R^2
null_model <- glm(lasso.data$outcome_binary ~ 1, family = "binomial")
null_deviance <- deviance(null_model)

# Calculate R^2
R2 <- 1 - (deviance / null_deviance)

# Print the results
cat("AIC:", AIC_value, "\n")
cat("BIC:", BIC_value, "\n")
cat("R^2:", R2, "\n")

new_X_matrix <- model.matrix(~ error_type * fb_type, data = new_X)
new_X_matrix <- new_X_matrix[, colnames(X)]

new_X$predicted_prob <- predict(model.RQ2.outcome,
                                newx = new_X_matrix,
                                s = "lambda.1se", type = "response")

new_X$predicted_prob <- plogis(new_X$predicted_prob)

m <- ggplot(new_X, aes(x = error_type,
                       y = predicted_prob,
                       color = fb_type,
                       group = fb_type)) +
  geom_line(alpha = 0.8) +
  geom_point() +
  labs(title = "Effect of error and WCF type on the predicted probability of error fixing",
       x = "\nError type",
       y = "Predicted probability\n(0 = Repeated error; 1 = Fixed error)\n") +
  theme_classic() +
  scale_color_brewer(palette = "Set2") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  guides(color = guide_legend(title = "WCF type"))

m


predicted_probs <- predict(model.RQ2.outcome, newx = model.matrix(~ error_type * fb_type, data = lasso.data), s = "lambda.1se", type = "response")

predicted_probs <- plogis(predicted_probs)

predicted_labels <- predict(model.RQ2.outcome, newx = model.matrix(~ error_type * fb_type, data = lasso.data), s = "lambda.1se", type = "response")

predicted_labels <- as.numeric(predicted_probs > 0.6097) # Median

conf_matrix <- table(Actual = lasso.data$outcome_binary, Predicted = predicted_labels)
conf_matrix

accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
accuracy
recall <- conf_matrix[2, 2] / sum(conf_matrix[2, ])
recall
precision <- conf_matrix[2, 2] / sum(conf_matrix[, 2])
precision
f1_score <- 2 * (precision * recall) / (precision + recall)
f1_score

prediction_obj <- prediction(predicted_labels, lasso.data$outcome_binary)
perf <- performance(prediction_obj, "tpr", "fpr")
plot(perf, main = "ROC Curve", col = "blue", lwd = 2)

auc <- performance(prediction_obj, "auc")
auc_value <- unlist(slot(auc, "y.values"))
auc_value

residuals_lasso <- lasso.data$outcome_binary - predicted_probs

residual_data <- data.frame(predicted_probs = predicted_probs,
                            fb_type = lasso.data$fb_type,
                            residuals = residuals_lasso)

r <- ggplot(residual_data, aes(x = predicted_probs,
                               y = residuals_lasso,
                               color = fb_type)) +
  geom_point() +
  labs(title = "Residuals vs Predicted Probabilities",
       x = "Predicted Probabilities",
       y = "Residuals")

r
```

<br>
<br>
<br>
**RQ3: HOW DO LEARNER HABITS, BELIEFS, AND PREFERENCES SHAPE THEIR ENGAGEMENT WITH DIFFERENT TYPES OF WCF, AND WHAT FACTORS CONTRIBUTE TO THEIR SUCCESSFUL APPROPRIATION?**

```{r rq3a, fig.height=6, fig.width=12, message=FALSE, results="verbatim", max.lines=1e6}
Sys.setenv(
  SDKROOT = system("xcrun --show-sdk-path", intern = TRUE),
  CC    = "clang",
  CXX   = "clang++",
  CXX11 = "clang++",
  CXX14 = "clang++",
  CXX17 = "clang++"
)

# === 3.1. DATA PREPARATION ===

# 3.1.1. Create clarity, usefulness, and liking columns

errors_and_preferences <- preferences_and_performance %>%
  dplyr::select(participant_ID,
                fb_importance,
                freq_seek_additional_info,
                how_much_fb_helps_learn,
                fb_1_clarity,
                fb_1_usefulness,
                likes_fb_1,
                fb_2_clarity,
                fb_2_usefulness,
                likes_fb_2,
                fb_4_clarity,
                fb_4_usefulness,
                likes_fb_4,
                fb_6_clarity,
                fb_6_usefulness,
                likes_fb_6,
                fb_7_clarity,
                fb_7_usefulness,
                likes_fb_7,
                error_type,
                fb_type,
                outcome,
                TextNum)

errors_and_preferences <- errors_and_preferences %>%
  rowwise() %>%
  mutate(clarity = case_when(
    fb_type == "Direct non-metalinguistic WCF" ~ fb_1_clarity,
    fb_type == "Direct metalinguistic WCF" ~ fb_2_clarity,
    fb_type == "Located indirect WCF" ~ fb_4_clarity,
    fb_type == "Indirect metalinguistic WCF" ~ fb_6_clarity,
    fb_type == "Reformulation" ~ fb_7_clarity,
    fb_type == "No WCF" ~ 0,
    TRUE ~ NA_real_
  ))

errors_and_preferences <- errors_and_preferences %>%
  rowwise() %>%
  mutate(usefulness = case_when(
    fb_type == "Direct non-metalinguistic WCF" ~ fb_1_usefulness,
    fb_type == "Direct metalinguistic WCF" ~ fb_2_usefulness,
    fb_type == "Located indirect WCF" ~ fb_4_usefulness,
    fb_type == "Indirect metalinguistic WCF" ~ fb_6_usefulness,
    fb_type == "Reformulation" ~ fb_7_usefulness,
    fb_type == "No WCF" ~ 0,
    TRUE ~ NA_real_
  ))

errors_and_preferences <- errors_and_preferences %>%
  rowwise() %>%
  mutate(liking = case_when(
    fb_type == "Direct non-metalinguistic WCF" ~ likes_fb_1,
    fb_type == "Direct metalinguistic WCF" ~ likes_fb_2,
    fb_type == "Located indirect WCF" ~ likes_fb_4,
    fb_type == "Indirect metalinguistic WCF" ~ likes_fb_6,
    fb_type == "Reformulation" ~ likes_fb_7,
    fb_type == "No WCF" ~ 0,
    TRUE ~ NA_real_
  ))

errors_and_preferences$outcome_binary <- ifelse(errors_and_preferences$outcome == "Fixed", 1, 0)


# 3.1.2. Calculate composite beliefs and preference scores

# Check Cronbach's α for each group of items

alpha_pref <- psych::alpha(errors_and_preferences[c("clarity",
                      "usefulness",
                      "liking")])

alpha_belief <- psych::alpha(errors_and_preferences[c("fb_importance", "how_much_fb_helps_learn")])

# Compute the composite scores

errors_and_preferences <- errors_and_preferences %>%
  ungroup() %>% # step 1: drop the rowwise grouping
  mutate(
    # Habits: there's only one indicator
    habit_score = freq_seek_additional_info,

    # Beliefs: two items
    belief_score = (fb_importance + how_much_fb_helps_learn) / 2,

    # Preferences: three items, matching this row's fb_type
    preference_score = (clarity + usefulness + liking) / 3
  )

errors_and_preferences <- errors_and_preferences %>%
  dplyr::select(participant_ID,
                belief_score,
                habit_score,
                preference_score,
                error_type,
                fb_type,
                outcome_binary,
                TextNum)


# 3.1.3. Scale habit, belief, and preference scores

scaled_learner_differences <- errors_and_preferences %>%
  mutate(across(
    .cols = c(habit_score, belief_score, preference_score),
    .fns = ~ as.numeric(scale(.)),
    .names = "{.col}_z"
  ))

# 3.1.4. Prepare data sets

# Pivot lmm.df long on the rate_fb_count_* columns
fb_cols <- grep("^rate_fb_count_", names(lmm.df), value = TRUE)

lmm_long <- lmm.df %>%
  pivot_longer(
    cols = all_of(fb_cols),
    names_to = "fb_type",
    values_to = "rate_scaled"
  ) %>%
  mutate(
    fb_type = case_when(
      fb_type == "rate_fb_count_D.M."  ~ "Direct non-metalinguistic WCF",
      fb_type == "rate_fb_count_D.N.M" ~ "Direct metalinguistic WCF",
      fb_type == "rate_fb_count_I.L."  ~ "Located indirect WCF",
      fb_type == "rate_fb_count_I.M."  ~ "Indirect metalinguistic WCF",
      fb_type == "rate_fb_count_R"     ~ "Reformulation",
      fb_type == "rate_fb_count_None"  ~ "No WCF",
      TRUE                             ~ fb_type
    ),
    errors_made = error_count,
    tokens_total = word_count
  ) %>%
  dplyr::select(participant_ID, TextNum, fb_type, errors_made, tokens_total)

# Join
scaled_learner_differences <- scaled_learner_differences %>% mutate(TextNum = as.character(TextNum))

df_mv <- scaled_learner_differences %>%
  left_join(lmm_long,
            by = c("participant_ID", "TextNum", "fb_type")
            )

df_mv$fb_type <- relevel(factor(df_mv$fb_type), ref = "No WCF")
df_mv$error_type <- factor(df_mv$error_type)

# 3.1.5. Fit the joint model in brms

priors.RQ3 <- c(
  # 1) Intercept on error-rate
  prior(normal(-3.22, 1), class = "Intercept", resp = "errorsmade"),
  # 2) WCF effects on errors_made (centered on RQ1 ORs)
  prior(normal(-0.04, 0.3), class = "b", coef = "fb_typeDirectmetalinguisticWCF", resp = "errorsmade"),
  prior(normal(0.03, 0.3), class = "b", coef = "fb_typeDirectnonMmetalinguisticWCF", resp = "errorsmade"),
  prior(normal(-0.13, 0.2), class = "b", coef = "fb_typeIndirectmetalinguisticWCF", resp = "errorsmade"),
  prior(normal(-0.03, 0.3), class = "b", coef = "fb_typeLocatedindirectWCF", resp = "errorsmade"),
  prior(normal(0.10, 0.3), class = "b", coef = "fb_typeReformulation", resp = "errorsmade"),
  # 3) All other slopes (e.g., habit_score_z, belief_score_z, ...) on each submodel
  prior(normal(0, 1), class = "b", resp = "errorsmade"),
  prior(normal(0, 1), class = "b", resp = "outcomebinary"),
  # 4) Random-effects scales informed by RQ1
  prior(student_t(3, 0, 0.27), class = "sd", group = "participant_ID", resp = "errorsmade"),
  prior(student_t(3, 0, 0.27), class = "sd", group = "participant_ID", resp = "outcomebinary"),
  # From:
  # df_mv %>%
  #  group_by(error_type) %>%
  #  summarize(
  #    prop = mean(errors_made / tokens_total),
  #    logit_prop = boot::logit(prop)
  #    ) %>%
  #  summarize(sd = sd(logit_prop))
  prior(student_t(3, 0, 0.25), class = "sd", group = "error_type", resp = "errorsmade"),
  # From:
  # df_mv %>%
  # group_by(error_type) %>%
  # summarize(
    # total trials and total successes per error type
    # n_trials    = sum(tokens_total),
    # n_successes = sum(outcome_binary)
  # ) %>%
  # mutate(
    # empirical logit with +0.5 / +1 smoothing
    # logit_success = log((n_successes + 0.5) / (n_trials - n_successes + 0.5))
  # ) %>%
  # summarize(
    # sd_logit = sd(logit_success)
  # )
  prior(student_t(3, 0, 0.57), class = "sd", group = "error_type", resp = "outcomebinary"),
  prior(student_t(3, 0, 0.15), class = "sd", group = "row_id", resp = "errorsmade"),
  prior(student_t(3, 0, 0.15), class = "sd", group = "row_id", resp = "outcomebinary"),
  # 5) Random-effects correlation
  prior(lkj(2), class = "cor")
)

# Add observation-level random intercept (OLRE) because beta-binomial cannot converge at a reasonable speed

df_mv$row_id <- 1:nrow(df_mv)

system.time({
  model.RQ3.appropriation <- brm(
  bf(errors_made | trials(tokens_total) ~ fb_type + habit_score_z + belief_score_z + preference_score_z + (1 | p | participant_ID) + (1 | e | error_type) + (1 | row_id)) +
    bf(outcome_binary ~ fb_type + habit_score_z + belief_score_z + preference_score_z + (1 | p | participant_ID) + (1 | e | error_type) + (1 | row_id)) + set_rescor(FALSE),
  data = df_mv,
  family = list(binomial(), bernoulli()),
  prior = priors.RQ3,
  cores = 4,
  chains = 4,
  refresh = 100,
  iter = 4000,
  warmup = 2000,
  seed = 123,
  save_pars  = save_pars(all = TRUE),
  control = list(
    adapt_delta = 0.995,
    max_treedepth = 20
  ))
})

summary(model.RQ3.appropriation)
```

```{r rq3b, results='asis', fig.height=6, fig.width=12}
#summary_lines <- capture.output(summary(model.RQ3.appropriation))
#cat('<div style="max-height:500px;overflow:auto;background:#f8f8f8;"><pre>',
#    paste(summary_lines, collapse="\n"),
#    '</pre></div>')

# out <- capture.output(summary(model.RQ3.appropriation))
# cat(out, sep = "\n")
# summary_out <- capture.output(summary(model.RQ3.appropriation))
# cat(head(summary_out, 200), sep = "\n")   # First 200 lines
# cat(tail(summary_out, 50), sep = "\n")    # Last 50 lines

```

```{r rq3c, fig.height=6, fig.width=12}

pp_check(model.RQ3.appropriation, resp = "errorsmade", type = "intervals_grouped", group = "participant_ID")
pp_check(model.RQ3.appropriation, resp = "errorsmade", type = "intervals_grouped", group = "error_type")

pp_check(model.RQ3.appropriation, resp = "errorsmade", type = "stat_2d", stat = c("mean", "var"))

bayes_R2(model.RQ3.appropriation, resp = "errorsmade")
bayes_R2(model.RQ3.appropriation, resp = "outcomebinary")

# Perform 10-fold cross-validation to get a robust estimate of out-of-sample predictive performance to avoid Pareto k issues altogether, since PSIS flagged 133 complicated observations when introducing overdispersion through OLRE

# Allow up to 1 GiB of globals

options(future.globals.maxSize = 1024^3)
#loo_mm <- loo(model.RQ3.appropriation)
#loo_mm # yields Warning: Found 133 observations with a pareto_k > 0.7 in model 'model.RQ3.appropriation'. With this many problematic observations, it may be more appropriate to use 'kfold' with argument 'K = 10' to perform 10-fold cross-validation rather than LOO.

kf10 <- kfold(model.RQ3.appropriation, K = 10)
kf10$estimates

kfoldic <- -2 * kf10$estimates["elpd_kfold", "Estimate"]
se_kfoldic <- 2 * kf10$estimates["elpd_kfold", "SE"]

kfoldic
se_kfoldic

```

```{r rq3d, fig.height=9, fig.width=6}

# Print most relevant PPC

mod <- model.RQ3.appropriation

# make each PPC
p_density <- pp_check(mod,
                      resp = "errorsmade",
                      type = "dens_overlay",
                      ndraws = 1000) +
  ggtitle("A: Error rate density") +
  theme_minimal()

p_rootogram <- pp_check(mod,
                        resp = "errorsmade",
                        type = "rootogram") +
  ggtitle("B: Error rate rootogram") +
  theme_minimal()

p_bars <- pp_check(mod,
                   resp = "outcomebinary",
                   type = "bars",
                   ndraws = 1000) +
  ggtitle("C: Fixed/repeated error counts") +
  theme_minimal()

# combine with patchwork
figure_ppc <- (p_density / p_rootogram / p_bars) +
  plot_annotation(
    title = "Posterior Predictive Checks for RQ3 model",
    theme = theme(plot.title = element_text(size = 14, face = "bold"))
  ) +
    plot_layout(
    ncol    = 1,
    heights = c(2, 2, 2.5)   # make the bottom plot a bit taller
  )

figure_ppc

```

